# âš¡ ShiftMaster - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦

ShiftMasteråŒ»ç™‚ã‚·ãƒ•ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã¯ã€24æ™‚é–“365æ—¥ç¨¼åƒã™ã‚‹åŒ»ç™‚æ©Ÿé–¢ã§ã®ä½¿ç”¨ã‚’æƒ³å®šã—ã€é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨å¯ç”¨æ€§ã‚’æä¾›ã™ã‚‹ã‚ˆã†è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™

### SLA (Service Level Agreement)

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ç›®æ¨™å€¤ | è¨±å®¹å€¤ | æ¸¬å®šæ–¹æ³• |
|-----------|--------|--------|----------|
| **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“** | < 200ms | < 500ms | P95 |
| **å¯ç”¨æ€§** | 99.9% | 99.5% | æœˆæ¬¡ |
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | 1000 req/sec | 500 req/sec | ãƒ”ãƒ¼ã‚¯æ™‚ |
| **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¿œç­”** | < 50ms | < 100ms | å¹³å‡ |
| **ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰æ™‚é–“** | < 2ç§’ | < 3ç§’ | First Contentful Paint |
| **APIå¿œç­”æ™‚é–“** | < 100ms | < 300ms | P99 |

### åŒ»ç™‚ã‚·ã‚¹ãƒ†ãƒ ç‰¹æœ‰è¦ä»¶

```yaml
medical_performance_requirements:
  emergency_response:
    target: "< 50ms"
    description: "ç·Šæ€¥æ™‚ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»é€šçŸ¥"
    
  shift_lookup:
    target: "< 100ms" 
    description: "å½“æ—¥ã‚·ãƒ•ãƒˆæ¤œç´¢"
    
  staff_availability:
    target: "< 200ms"
    description: "ã‚¹ã‚¿ãƒƒãƒ•ç©ºãçŠ¶æ³ç¢ºèª"
    
  schedule_generation:
    target: "< 5ç§’"
    description: "æœˆæ¬¡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è‡ªå‹•ç”Ÿæˆ"
    
  compliance_reporting:
    target: "< 10ç§’"
    description: "è¦åˆ¶ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"
```

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æœ€é©åŒ–

### ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹åˆ†æ•£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
graph TB
    A[Load Balancer] --> B[API Gateway]
    B --> C[Authentication Service]
    B --> D[Shift Management Service]
    B --> E[Staff Management Service]
    B --> F[Notification Service]
    
    D --> G[PostgreSQL Primary]
    D --> H[PostgreSQL Read Replica]
    E --> G
    E --> H
    
    D --> I[Redis Cache]
    E --> I
    F --> J[Message Queue]
    
    K[CDN] --> L[Static Assets]
    M[Elasticsearch] --> N[Search Index]
```

### æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥

```python
# è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è¨­å®š
AUTO_SCALING_CONFIG = {
    "web_servers": {
        "min_instances": 2,
        "max_instances": 10,
        "target_cpu_utilization": 70,
        "scale_up_threshold": 80,
        "scale_down_threshold": 30,
        "cooldown_period": 300  # ç§’
    },
    "database": {
        "read_replicas": {
            "min": 1,
            "max": 3,
            "auto_scale_by_connections": True
        },
        "connection_pooling": {
            "max_connections": 200,
            "min_connections": 20
        }
    },
    "cache": {
        "redis_cluster": {
            "nodes": 3,
            "memory_per_node": "4GB",
            "eviction_policy": "allkeys-lru"
        }
    }
}

class AutoScaler:
    """è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç®¡ç†"""
    
    def monitor_and_scale(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒ»ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
        metrics = self.get_current_metrics()
        
        # CPUä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if metrics["cpu_usage"] > 80:
            self.scale_up_web_servers()
        elif metrics["cpu_usage"] < 30:
            self.scale_down_web_servers()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæ•°ãƒã‚§ãƒƒã‚¯
        if metrics["db_connections"] > 150:
            self.add_read_replica()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if metrics["memory_usage"] > 85:
            self.increase_cache_memory()
```

## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–

### ã‚¯ã‚¨ãƒªæœ€é©åŒ–

```python
# åŠ¹ç‡çš„ãªã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³
class OptimizedShiftQueries:
    """æœ€é©åŒ–ã•ã‚ŒãŸã‚·ãƒ•ãƒˆã‚¯ã‚¨ãƒª"""
    
    @staticmethod
    def get_weekly_schedule(start_date, department_id):
        """é€±é–“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        return Shift.objects.select_related(
            'staff__user',
            'department'
        ).prefetch_related(
            'staff__qualifications'
        ).filter(
            date__range=(start_date, start_date + timedelta(days=7)),
            department_id=department_id
        ).order_by('date', 'start_time')
    
    @staticmethod
    def get_staff_availability(date, department_id):
        """ã‚¹ã‚¿ãƒƒãƒ•ç©ºãçŠ¶æ³ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        # ã‚µãƒ–ã‚¯ã‚¨ãƒªã§ã‚·ãƒ•ãƒˆä¸­ã‚¹ã‚¿ãƒƒãƒ•ã‚’é™¤å¤–
        busy_staff = Shift.objects.filter(
            date=date
        ).values_list('staff_id', flat=True)
        
        return Staff.objects.select_related(
            'user', 'department'
        ).filter(
            department_id=department_id,
            is_active=True
        ).exclude(
            id__in=busy_staff
        ).annotate(
            qualification_count=Count('qualifications')
        ).order_by('-qualification_count')
    
    @staticmethod
    def get_department_coverage(date_range):
        """éƒ¨ç½²ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        return Shift.objects.values(
            'department__name',
            'date'
        ).annotate(
            total_hours=Sum(
                Extract('hour', F('end_time') - F('start_time'))
            ),
            staff_count=Count('staff', distinct=True)
        ).filter(
            date__range=date_range
        ).order_by('department__name', 'date')
```

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥

```sql
-- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX CONCURRENTLY idx_shifts_date_department 
ON shifts_shift (date, department_id);

CREATE INDEX CONCURRENTLY idx_shifts_staff_date 
ON shifts_shift (staff_id, date);

CREATE INDEX CONCURRENTLY idx_shifts_datetime_range 
ON shifts_shift (date, start_time, end_time);

-- è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ¤œç´¢é »åº¦é«˜ï¼‰
CREATE INDEX CONCURRENTLY idx_staff_department_active 
ON shifts_staff (department_id, is_active) 
WHERE is_active = true;

-- éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¹ã‚¿ãƒƒãƒ•ã®ã¿ï¼‰
CREATE INDEX CONCURRENTLY idx_staff_active_role 
ON shifts_staff (role, created_at) 
WHERE is_active = true;

-- JSONãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX CONCURRENTLY idx_staff_qualifications_gin 
ON shifts_staff USING gin (qualifications);

-- å…¨æ–‡æ¤œç´¢ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX CONCURRENTLY idx_staff_search_vector 
ON shifts_staff USING gin (search_vector);
```

### æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–

```python
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'shiftmaster',
        'USER': 'shiftmaster_user',
        'PASSWORD': os.environ['DB_PASSWORD'],
        'HOST': 'db-primary',
        'PORT': '5432',
        'OPTIONS': {
            # æ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
            'MAX_CONNS': 200,
            'MIN_CONNS': 20,
            
            # æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            'CONN_MAX_AGE': 300,
            'CONN_HEALTH_CHECKS': True,
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
            'autocommit': True,
            'isolation_level': psycopg2.extensions.ISOLATION_LEVEL_READ_COMMITTED,
            
            # ãƒãƒƒãƒ•ã‚¡è¨­å®š
            'shared_preload_libraries': 'pg_stat_statements',
            'effective_cache_size': '4GB',
            'work_mem': '16MB',
        }
    },
    'read_replica': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'shiftmaster',
        'USER': 'shiftmaster_readonly',
        'PASSWORD': os.environ['DB_READONLY_PASSWORD'],
        'HOST': 'db-replica',
        'PORT': '5432',
        'OPTIONS': {
            'MAX_CONNS': 100,
            'MIN_CONNS': 10,
        }
    }
}

# èª­ã¿å–ã‚Šå°‚ç”¨ã‚¯ã‚¨ãƒªã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
class DatabaseRouter:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    
    read_only_models = [
        'reports', 'analytics', 'logs'
    ]
    
    def db_for_read(self, model, **hints):
        """èª­ã¿å–ã‚Šã‚¯ã‚¨ãƒªã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
        if model._meta.app_label in self.read_only_models:
            return 'read_replica'
        
        # é‡ã„ã‚¯ã‚¨ãƒªã¯ãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«ã¸
        if hints.get('heavy_query'):
            return 'read_replica'
        
        return 'default'
    
    def db_for_write(self, model, **hints):
        """æ›¸ãè¾¼ã¿ã‚¯ã‚¨ãƒªã¯å¸¸ã«ãƒ—ãƒ©ã‚¤ãƒãƒª"""
        return 'default'
```

## ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

### å¤šå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```python
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
CACHES = {
    'default': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://redis-cluster:6379/0',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
            'CONNECTION_POOL_KWARGS': {
                'max_connections': 50,
                'health_check_interval': 30,
            },
            'COMPRESSOR': 'django_redis.compressors.zlib.ZlibCompressor',
            'SERIALIZER': 'django_redis.serializers.msgpack.MSGPackSerializer',
        },
        'TIMEOUT': 300,
        'KEY_PREFIX': 'shiftmaster',
        'VERSION': 1,
    },
    'session': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://redis-cluster:6379/1',
        'TIMEOUT': 3600,
    },
    'long_term': {
        'BACKEND': 'django_redis.cache.RedisCache',
        'LOCATION': 'redis://redis-cluster:6379/2',
        'TIMEOUT': 86400,  # 24æ™‚é–“
    }
}

class SmartCacheManager:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†"""
    
    def __init__(self):
        self.cache = cache
        self.long_cache = caches['long_term']
    
    def get_or_set_with_tags(self, key, factory_func, timeout=300, tags=None):
        """ã‚¿ã‚°ä»˜ãã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        data = self.cache.get(key)
        if data is None:
            data = factory_func()
            self.cache.set(key, data, timeout)
            
            # ã‚¿ã‚°ç®¡ç†
            if tags:
                for tag in tags:
                    self.add_to_tag_set(tag, key)
        
        return data
    
    def invalidate_by_tag(self, tag):
        """ã‚¿ã‚°ã«ã‚ˆã‚‹ä¸€æ‹¬ç„¡åŠ¹åŒ–"""
        keys = self.get_keys_by_tag(tag)
        self.cache.delete_many(keys)
        self.clear_tag_set(tag)
    
    def warm_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—"""
        # ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.warm_department_data()
        self.warm_staff_data()
        self.warm_current_week_shifts()
    
    def warm_department_data(self):
        """éƒ¨ç½²ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        departments = Department.objects.select_related().all()
        for dept in departments:
            cache_key = f"department:{dept.id}"
            self.long_cache.set(cache_key, dept, 86400)
    
    def warm_current_week_shifts(self):
        """ä»Šé€±ã®ã‚·ãƒ•ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        today = timezone.now().date()
        week_start = today - timedelta(days=today.weekday())
        
        shifts = Shift.objects.select_related(
            'staff__user', 'department'
        ).filter(
            date__range=(week_start, week_start + timedelta(days=7))
        )
        
        cache_key = f"current_week_shifts:{week_start}"
        self.cache.set(cache_key, list(shifts), 3600)
```

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥

```python
# ãƒ“ãƒ¥ãƒ¼ãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
from django.views.decorators.cache import cache_page
from django.utils.decorators import method_decorator

@method_decorator(cache_page(300), name='get')  # 5åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
class ShiftListView(ListView):
    """ã‚·ãƒ•ãƒˆä¸€è¦§ãƒ“ãƒ¥ãƒ¼ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    model = Shift
    
    def get_cache_key(self):
        """å‹•çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        params = self.request.GET
        user_dept = self.request.user.department_id
        return f"shift_list:{user_dept}:{hash(frozenset(params.items()))}"

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
@register.inclusion_tag('shifts/shift_card.html')
def shift_card(shift):
    """ã‚·ãƒ•ãƒˆã‚«ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    cache_key = f"shift_card:{shift.id}:{shift.updated_at.timestamp()}"
    return cache.get_or_set(cache_key, lambda: {
        'shift': shift,
        'staff_name': shift.staff.get_full_name(),
        'department': shift.department.name,
    }, 3600)

# APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥
class CachedAPIView(APIView):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãAPIãƒ“ãƒ¥ãƒ¼"""
    cache_timeout = 300
    
    def get(self, request, *args, **kwargs):
        cache_key = self.get_cache_key(request, *args, **kwargs)
        
        response_data = cache.get(cache_key)
        if response_data is None:
            response_data = self.get_response_data(request, *args, **kwargs)
            cache.set(cache_key, response_data, self.cache_timeout)
        
        return Response(response_data)
    
    def get_cache_key(self, request, *args, **kwargs):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå›ºæœ‰ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼"""
        url_params = request.query_params.dict()
        user_id = request.user.id
        return f"api:{self.__class__.__name__}:{user_id}:{hash(frozenset(url_params.items()))}"
```

## ğŸ¨ ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰æœ€é©åŒ–

### ã‚¢ã‚»ãƒƒãƒˆæœ€é©åŒ–

```javascript
// Webpackè¨­å®šï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰
const path = require('path');
const MiniCssExtractPlugin = require('mini-css-extract-plugin');
const TerserPlugin = require('terser-webpack-plugin');
const CompressionPlugin = require('compression-webpack-plugin');

module.exports = {
  mode: 'production',
  
  optimization: {
    minimizer: [
      new TerserPlugin({
        parallel: true,
        terserOptions: {
          compress: {
            drop_console: true,
            drop_debugger: true,
          },
        },
      }),
    ],
    
    splitChunks: {
      chunks: 'all',
      cacheGroups: {
        vendor: {
          test: /[\\/]node_modules[\\/]/,
          name: 'vendors',
          priority: 10,
        },
        common: {
          name: 'common',
          minChunks: 2,
          priority: 5,
        },
      },
    },
  },
  
  plugins: [
    new MiniCssExtractPlugin({
      filename: '[name].[contenthash].css',
      chunkFilename: '[id].[contenthash].css',
    }),
    
    new CompressionPlugin({
      algorithm: 'gzip',
      test: /\.(js|css|html|svg)$/,
      threshold: 8192,
      minRatio: 0.8,
    }),
  ],
  
  module: {
    rules: [
      {
        test: /\.js$/,
        exclude: /node_modules/,
        use: {
          loader: 'babel-loader',
          options: {
            presets: ['@babel/preset-env'],
            plugins: ['@babel/plugin-syntax-dynamic-import'],
          },
        },
      },
      
      {
        test: /\.(png|jpg|gif|svg)$/,
        use: [
          {
            loader: 'url-loader',
            options: {
              limit: 8192,
              name: '[name].[hash].[ext]',
              outputPath: 'images/',
            },
          },
          {
            loader: 'image-webpack-loader',
            options: {
              mozjpeg: { progressive: true, quality: 85 },
              optipng: { enabled: true },
              pngquant: { quality: [0.65, 0.90], speed: 4 },
              gifsicle: { interlaced: false },
            },
          },
        ],
      },
    ],
  },
};
```

### ãƒ¬ã‚¤ã‚¸ãƒ¼ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

```javascript
// ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ¬ã‚¤ã‚¸ãƒ¼ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
class ShiftCalendar {
    constructor() {
        this.loadCalendarModule();
    }
    
    async loadCalendarModule() {
        // å¤§ããªã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’é…å»¶èª­ã¿è¾¼ã¿
        const { Calendar } = await import('./modules/calendar');
        this.calendar = new Calendar();
        this.initializeCalendar();
    }
    
    async loadShiftData(date) {
        // ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ®µéšçš„èª­ã¿è¾¼ã¿
        const visibleRange = this.getVisibleDateRange();
        const shifts = await this.fetchShifts(visibleRange);
        this.renderShifts(shifts);
    }
    
    // Intersection Observer for lazy loading
    setupLazyLoading() {
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    this.loadShiftDetails(entry.target);
                    observer.unobserve(entry.target);
                }
            });
        });
        
        document.querySelectorAll('.shift-card').forEach(card => {
            observer.observe(card);
        });
    }
}

// ç”»åƒé…å»¶èª­ã¿è¾¼ã¿
class LazyImageLoader {
    static init() {
        if ('IntersectionObserver' in window) {
            const imageObserver = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        const img = entry.target;
                        img.src = img.dataset.src;
                        img.classList.remove('lazy');
                        imageObserver.unobserve(img);
                    }
                });
            });
            
            document.querySelectorAll('img[data-src]').forEach(img => {
                imageObserver.observe(img);
            });
        }
    }
}
```

### Critical CSS ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³åŒ–

```python
# Critical CSS æŠ½å‡ºãƒ»ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³åŒ–
def generate_critical_css(url):
    """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«CSSç”Ÿæˆ"""
    import subprocess
    
    # Puppeteerã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«CSSæŠ½å‡º
    result = subprocess.run([
        'node', 'extract-critical-css.js',
        '--url', url,
        '--width', '1300',
        '--height', '900'
    ], capture_output=True, text=True)
    
    return result.stdout

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«CSSä½¿ç”¨
class OptimizedTemplateView(TemplateView):
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ“ãƒ¥ãƒ¼"""
    
    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        
        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«CSSã‚’ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³åŒ–
        if not settings.DEBUG:
            critical_css = cache.get_or_set(
                f'critical_css:{self.template_name}',
                lambda: self.get_critical_css(),
                86400  # 24æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            )
            context['critical_css'] = critical_css
        
        return context
    
    def get_critical_css(self):
        """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«CSSå–å¾—"""
        current_url = self.request.build_absolute_uri()
        return generate_critical_css(current_url)
```

## ğŸ“± ãƒ¢ãƒã‚¤ãƒ«æœ€é©åŒ–

### PWAå¯¾å¿œ

```javascript
// Service Worker for PWA
self.addEventListener('install', event => {
    event.waitUntil(
        caches.open('shiftmaster-v1').then(cache => {
            return cache.addAll([
                '/',
                '/static/css/app.css',
                '/static/js/app.js',
                '/shifts/',
                '/offline.html'
            ]);
        })
    );
});

self.addEventListener('fetch', event => {
    // ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å„ªå…ˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    event.respondWith(
        fetch(event.request)
            .then(response => {
                // æˆåŠŸæ™‚ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
                if (response.status === 200) {
                    const responseClone = response.clone();
                    caches.open('shiftmaster-v1').then(cache => {
                        cache.put(event.request, responseClone);
                    });
                }
                return response;
            })
            .catch(() => {
                // ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æä¾›
                return caches.match(event.request)
                    .then(response => {
                        return response || caches.match('/offline.html');
                    });
            })
    );
});

// ãƒ—ãƒƒã‚·ãƒ¥é€šçŸ¥
self.addEventListener('push', event => {
    const options = {
        body: event.data.text(),
        icon: '/static/icons/notification-icon.png',
        badge: '/static/icons/badge.png',
        vibrate: [100, 50, 100],
        data: {
            url: '/shifts/'
        },
        actions: [
            {
                action: 'view',
                title: 'ã‚·ãƒ•ãƒˆç¢ºèª'
            },
            {
                action: 'close',
                title: 'é–‰ã˜ã‚‹'
            }
        ]
    };
    
    event.waitUntil(
        self.registration.showNotification('ShiftMaster', options)
    );
});
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ç”»åƒ

```html
<!-- ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ç”»åƒé…ä¿¡ -->
<picture>
    <source 
        media="(max-width: 768px)" 
        srcset="/static/images/staff-photo-small.webp 1x, 
                /static/images/staff-photo-small@2x.webp 2x"
        type="image/webp">
    <source 
        media="(max-width: 768px)" 
        srcset="/static/images/staff-photo-small.jpg 1x, 
                /static/images/staff-photo-small@2x.jpg 2x">
    <source 
        srcset="/static/images/staff-photo.webp 1x, 
                /static/images/staff-photo@2x.webp 2x"
        type="image/webp">
    <img 
        src="/static/images/staff-photo.jpg" 
        srcset="/static/images/staff-photo@2x.jpg 2x"
        alt="ã‚¹ã‚¿ãƒƒãƒ•å†™çœŸ"
        loading="lazy">
</picture>
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
# ã‚«ã‚¹ã‚¿ãƒ ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
class PerformanceMonitoringMiddleware:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢"""
    
    def __init__(self, get_response):
        self.get_response = get_response
    
    def __call__(self, request):
        start_time = time.time()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªæ•°ã‚’ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
        initial_queries = len(connection.queries)
        
        response = self.get_response(request)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        response_time = time.time() - start_time
        query_count = len(connection.queries) - initial_queries
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        self.record_metrics(request, response, response_time, query_count)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ 
        response['X-Response-Time'] = f"{response_time:.3f}s"
        response['X-Query-Count'] = str(query_count)
        
        return response
    
    def record_metrics(self, request, response, response_time, query_count):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
        # Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹
        REQUEST_DURATION.labels(
            method=request.method,
            endpoint=request.path,
            status=response.status_code
        ).observe(response_time)
        
        QUERY_COUNT.labels(
            endpoint=request.path
        ).observe(query_count)
        
        # é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if response_time > 1.0:  # 1ç§’ä»¥ä¸Š
            logger.warning(
                f"Slow request: {request.path} took {response_time:.3f}s "
                f"with {query_count} queries"
            )
        
        if query_count > 10:  # 10ã‚¯ã‚¨ãƒªä»¥ä¸Š
            logger.warning(
                f"High query count: {request.path} executed {query_count} queries"
            )
```

### APMãƒ„ãƒ¼ãƒ«çµ±åˆ

```python
# New Relic / DataDog çµ±åˆ
import newrelic.agent

@newrelic.agent.function_trace()
def generate_monthly_schedule(department_id, year, month):
    """æœˆæ¬¡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç”Ÿæˆï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹ä»˜ãï¼‰"""
    with newrelic.agent.BackgroundTask(application, 'schedule_generation'):
        # è¤‡é›‘ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
        pass

# ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
def record_business_metrics():
    """ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
    active_users = User.objects.filter(
        last_login__gte=timezone.now() - timedelta(hours=24)
    ).count()
    newrelic.agent.record_custom_metric('Custom/ActiveUsers', active_users)
    
    # ä»Šæ—¥ã®ã‚·ãƒ•ãƒˆæ•°
    todays_shifts = Shift.objects.filter(date=timezone.now().date()).count()
    newrelic.agent.record_custom_metric('Custom/TodaysShifts', todays_shifts)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º
    db_size = get_database_size()
    newrelic.agent.record_custom_metric('Custom/DatabaseSize', db_size)
```

### ãƒ­ã‚°åˆ†æ

```python
# æ§‹é€ åŒ–ãƒ­ã‚°è¨­å®š
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'json': {
            'format': '%(asctime)s %(name)s %(levelname)s %(message)s',
            'class': 'pythonjsonlogger.jsonlogger.JsonFormatter',
        },
        'performance': {
            'format': 'PERF %(asctime)s %(name)s %(message)s',
        },
    },
    'handlers': {
        'performance_file': {
            'level': 'INFO',
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': 'logs/performance.log',
            'maxBytes': 50 * 1024 * 1024,  # 50MB
            'backupCount': 10,
            'formatter': 'json',
        },
    },
    'loggers': {
        'performance': {
            'handlers': ['performance_file'],
            'level': 'INFO',
            'propagate': False,
        },
    },
}

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°è¨˜éŒ²
performance_logger = logging.getLogger('performance')

def log_performance_event(event_type, **kwargs):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°"""
    performance_logger.info('', extra={
        'event_type': event_type,
        'timestamp': time.time(),
        **kwargs
    })
```

## âš¡ ç¶™ç¶šçš„æœ€é©åŒ–

### è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```python
# Locustã‚’ä½¿ç”¨ã—ãŸç¶™ç¶šçš„è² è·ãƒ†ã‚¹ãƒˆ
from locust import HttpUser, task, between

class ShiftMasterLoadTest(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        # ãƒ­ã‚°ã‚¤ãƒ³
        response = self.client.post("/login/", {
            "username": "test@hospital.com",
            "password": "testpass123"
        })
    
    @task(3)
    def view_today_shifts(self):
        """ä»Šæ—¥ã®ã‚·ãƒ•ãƒˆè¡¨ç¤ºï¼ˆé«˜é »åº¦ï¼‰"""
        self.client.get("/shifts/today/")
    
    @task(2)
    def view_weekly_schedule(self):
        """é€±é–“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¡¨ç¤º"""
        self.client.get("/shifts/week/")
    
    @task(1)
    def search_staff(self):
        """ã‚¹ã‚¿ãƒƒãƒ•æ¤œç´¢"""
        self.client.get("/staff/", params={"department": "å†…ç§‘"})
    
    @task(1)
    def create_shift(self):
        """ã‚·ãƒ•ãƒˆä½œæˆï¼ˆé‡ã„å‡¦ç†ï¼‰"""
        self.client.post("/shifts/create/", {
            "staff": 1,
            "date": "2024-01-20",
            "start_time": "09:00",
            "end_time": "17:00"
        })

# è‡ªå‹•ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
def run_performance_tests():
    """è‡ªå‹•ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    import subprocess
    
    # è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    result = subprocess.run([
        'locust',
        '-f', 'performance_tests/locustfile.py',
        '--headless',
        '--users', '100',
        '--spawn-rate', '10',
        '--run-time', '5m',
        '--html', 'reports/performance_report.html'
    ], capture_output=True, text=True)
    
    # çµæœåˆ†æ
    analyze_performance_results()
```

### è‡ªå‹•æœ€é©åŒ–

```bash
#!/bin/bash
# è‡ªå‹•æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆæ›´æ–°
psql -d shiftmaster -c "ANALYZE;"

# æœªä½¿ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œå‡º
python manage.py detect_unused_indexes

# é…ã„ã‚¯ã‚¨ãƒªåˆ†æ
python manage.py analyze_slow_queries

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ç¢ºèª
python manage.py cache_statistics

# è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ææ¡ˆ
python manage.py suggest_optimizations
```

---

**ã“ã®åŒ…æ‹¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã«ã‚ˆã‚Šã€ShiftMasterã¯åŒ»ç™‚æ©Ÿé–¢ã®å³ã—ã„æ€§èƒ½è¦ä»¶ã‚’æº€ãŸã—ã¾ã™ã€‚**

**æœ€çµ‚æ›´æ–°**: 2024å¹´1æœˆ20æ—¥  
**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0
